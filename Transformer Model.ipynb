{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92091a6c-ce4c-4945-84da-a683d3bb069b",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3dcbad-5ef7-4102-b171-7ed30a7eee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import pickle\n",
    "import pydicom\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb10275-f290-4234-91bd-f14b973a6fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 4090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e98d081-742f-4513-839b-94cdb66ad183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer.attention import attention\n",
    "from Transformer.DecoderLayer import Decoder_Layer\n",
    "from Transformer.multiHeadAttention import MultiHeadAttention\n",
    "from Transformer.utils import positional_encoding\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04fcb650-24f3-42f3-94da-4c58ea4ac5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getParameters import get_gait_parameters_insole\n",
    "from getParameters import gait_aligned_jnt\n",
    "from pre import butter_lowpass_filter\n",
    "from DataManager import DataManager\n",
    "import scipy.io as sio\n",
    "from scipy.signal import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdaffae-2588-4aec-a06a-6932fc3566c8",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6f7f1c-b04f-4876-89bf-be4586b3c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "DM = DataManager()\n",
    "available_ID_list = DM.ID_list #Get all available list\n",
    "available_ID_list = ['GAIT102424-01', 'GAIT102424-02'] # or type in manually\n",
    "\n",
    "valid_files = DM.get_valid_data_file_dict(available_ID_list)\n",
    "\n",
    "# Load Feature Names\n",
    "names = ['foot_trace_r', 'cop_x_r', 'cop_y_r', 'cont_area_r', 'pp_r', 'pp_x_r', 'pp_y_r',\n",
    "             'foot_trace_l', 'cop_x_l', 'cop_y_l', 'cont_area_l', 'pp_l', 'pp_x_l', 'pp_y_l'] # matrix name\n",
    "\n",
    "# Create feature list to store calculation results\n",
    "FM_all = []\n",
    "ankle_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3bab654-d88d-4771-ba73-22f6f6b0649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in valid_files:\n",
    "    FM_all = []\n",
    "    ankle_all = []\n",
    "    data = sio.loadmat(file['path'])\n",
    "    insoleAll_l = data['insoleAll_l'].astype(np.float64)\n",
    "    insoleAll_r = data['insoleAll_r'].astype(np.float64)\n",
    "    t_insole_l = data['t_insole_l'].astype(np.float64)\n",
    "    t_insole_r = data['t_insole_r'].astype(np.float64)\n",
    "\n",
    "    t_trackers = data['t_trackers'].astype(np.float64)\n",
    "    jnt_angles_all_l = np.array(data['jnt_angles_all_l'])\n",
    "    jnt_angles_all_r = np.array(data['jnt_angles_all_r'])\n",
    "    jnt_pos_all_l = np.array(data['jnt_pos_all_l'])\n",
    "    jnt_pos_all_r = np.array(data['jnt_pos_all_r'])\n",
    "\n",
    "    # feature matrix\n",
    "    g = get_gait_parameters_insole(insoleAll_r, insoleAll_l, t_insole_r, t_insole_l)\n",
    "    FM_r = [g['foot_trace_r'],g['cop_x_r'],g['cop_y_r'],g['cont_area_r'],g['pp_r'],g['pp_x_r'],g['pp_y_r']]\n",
    "    FM_l = [g['foot_trace_l'],g['cop_x_l'],g['cop_y_l'],g['cont_area_l'],g['pp_l'],g['pp_x_l'],g['pp_y_l']]\n",
    "    FM_r, FM_l = np.column_stack(FM_r), np.column_stack(FM_l)\n",
    "\n",
    "    # resample\n",
    "    fm = []\n",
    "    step_num = len(g['strike_l'])\n",
    "    for step in range(10,step_num - 10):\n",
    "        start_i, end_i = g['strike_l'][step], g['strike_l'][step+1]\n",
    "        fm_l=resample(FM_l[start_i:end_i,:], 100, axis=0)\n",
    "        fm_r=resample(FM_r[start_i:end_i, :], 100, axis=0)\n",
    "        fm.append(np.hstack((fm_l, fm_r)))\n",
    "    FM = np.vstack(fm)\n",
    "\n",
    "    joint = gait_aligned_jnt(g, jnt_angles_all_l, jnt_angles_all_r, jnt_pos_all_l, jnt_pos_all_r, t_trackers)\n",
    "    angle = joint['resampled_angles_l']['ankle'][100*9:109*100,:]\n",
    "\n",
    "    for i in range(FM.shape[0]):\n",
    "        FM_all.append(FM[i, :])\n",
    "        ankle_all.append(angle[i, :])\n",
    "\n",
    "    feature_matrix = np.array(FM_all)\n",
    "    ankle_matrix = np.array(ankle_all)\n",
    "    \n",
    "    trail_num = file['meta_data']['Trial'][0]\n",
    "    feature_path = file['path'].with_name(file['path'].stem + '_feature.npy')\n",
    "    ankle_angle_path = file['path'].with_name(file['path'].stem + '_ankle_angle.npy')\n",
    "    \n",
    "    # np.save(feature_path, feature_matrix)\n",
    "    # np.save(ankle_angle_path, ankle_matrix)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580d511-e17a-4b76-a637-2a09153dafb9",
   "metadata": {},
   "source": [
    "# VisTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91922bcb-a658-4ebc-8165-b11aa632339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_dim:\n",
    "        Number of image dimension of tuple (w*h)\n",
    "    patch_num:\n",
    "        Number of patches used in each row and column\n",
    "    d_base:\n",
    "        Number of baseline / time-independent covariates\n",
    "    d_model:\n",
    "        Dimension of the input vector (post embedding)\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    num_decoder_layers:\n",
    "        Number of decoder layers to stack\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_dim,\n",
    "                 n_patch,\n",
    "                 d_output,\n",
    "                 d_model = 32,\n",
    "                 nhead = 4,\n",
    "                 num_decoder_layers = 4,\n",
    "                 dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_width = int(image_dim[0]/n_patch)\n",
    "\n",
    "        self.patch_height = int(image_dim[1]/n_patch)\n",
    "\n",
    "        self.n_patch = n_patch\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([Decoder_Layer(d_model,nhead,dropout)\n",
    "                                             for _ in range(num_decoder_layers)])\n",
    "        \n",
    "        self.RELU = nn.ReLU()\n",
    "\n",
    "        self.patch_embedding = nn.Linear(self.patch_width*self.patch_height, d_model)\n",
    "\n",
    "        self.pred = nn.Linear(n_patch*d_model, d_output)\n",
    "\n",
    "\n",
    "    def forward(self, patches):\n",
    "        \"\"\"\n",
    "        patches: size of batch*num_patches*patch_width*patch_height\n",
    "        \"\"\"\n",
    "\n",
    "        flatten_patches = patches.reshape(patches.shape[0], patches.shape[1], self.patch_width*self.patch_height)\n",
    "\n",
    "        # Patch Embedding to model demension\n",
    "        x = self.patch_embedding(flatten_patches)\n",
    "\n",
    "        # Positional Embedding using index of patches\n",
    "        x = x + positional_encoding(x.shape[0], x.shape[1], x.shape[2])\n",
    "\n",
    "        # Identical Mask (No information is masked)\n",
    "        mask = torch.ones((x.shape[0],x.shape[1],x.shape[1])).to(device)\n",
    "        \n",
    "        # Decoder Layers\n",
    "        # Decoder Layer with prediction time embedding\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, x, mask) \n",
    "        \n",
    "        # Flatten the attention matrix\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.pred(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636e46d-de07-40a8-bb0d-0a67f78d01d9",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24f3358e-aff3-4226-8510-bee84004a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insole_input = torch.randn(10,4,16,4).to(device) # batch * n_patch * patch_dim_height * patch_dim_width\n",
    "insole_output = torch.randn(10,1).to(device) # batch * joint_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbda3e95-64c7-4dc7-a92d-7245784677c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9346],\n",
       "        [-0.3099],\n",
       "        [-0.6195],\n",
       "        [ 0.3713],\n",
       "        [-0.5137],\n",
       "        [-0.4942],\n",
       "        [-0.1884],\n",
       "        [-0.5465],\n",
       "        [-0.1173],\n",
       "        [-0.8626]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisualTransformer(n_patch = 4, d_model=32, image_dim = (64,16), d_output = 1).to(device)\n",
    "model(insole_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
